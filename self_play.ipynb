{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d4159ae",
   "metadata": {
    "id": "2d4159ae"
   },
   "source": [
    "# Checkers AI training through self play"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640e8222",
   "metadata": {
    "id": "640e8222"
   },
   "source": [
    "This notebook uses a checkers implementation from the pettingzoo package. \n",
    "\n",
    "Monte carlo tree search and self-play training is based on https://web.stanford.edu/~surag/posts/alphazero.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7FAeqZUzFABQ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7FAeqZUzFABQ",
    "outputId": "6545d77a-1037-4ccf-a59e-a49045ecef60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pettingzoo in ./.venv/lib/python3.8/site-packages (1.13.1)\n",
      "Requirement already satisfied: gym>=0.21.0 in ./.venv/lib/python3.8/site-packages (from pettingzoo) (0.21.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in ./.venv/lib/python3.8/site-packages (from pettingzoo) (1.21.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in ./.venv/lib/python3.8/site-packages (from gym>=0.21.0->pettingzoo) (2.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pettingzoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdc28385",
   "metadata": {
    "id": "fdc28385"
   },
   "outputs": [],
   "source": [
    "from pettingzoo.utils.env import AECEnv\n",
    "from pettingzoo.classic import checkers_v3\n",
    "from copy import deepcopy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafa1d70",
   "metadata": {
    "id": "dafa1d70"
   },
   "source": [
    "## Utils\n",
    "Let's start by defining some util classes used later in the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d50f5b9b",
   "metadata": {
    "id": "d50f5b9b"
   },
   "outputs": [],
   "source": [
    "# Game env wrapper for MCTS search\n",
    "class State:\n",
    "\n",
    "    def __init__(self, env : AECEnv):\n",
    "        self.env = env\n",
    "\n",
    "    def gameEnded(self):\n",
    "        _, _, done, _ = self.env.last()\n",
    "        return done\n",
    "\n",
    "    def gameReward(self):\n",
    "        _, reward, _, _ = self.env.last()\n",
    "        return reward\n",
    "\n",
    "    def getActionMask(self):\n",
    "        observation, _, _, _ = self.env.last()\n",
    "        return observation[\"action_mask\"]\n",
    "\n",
    "    def getValidActions(self):\n",
    "        return np.flatnonzero(self.getActionMask())\n",
    "\n",
    "    def nextState(self, action):\n",
    "        new_env = deepcopy(self.env)\n",
    "        new_env.step(action)\n",
    "        player_changed = self.env.agent_selection != new_env.agent_selection\n",
    "        return State(new_env), player_changed\n",
    "\n",
    "    def getObservation(self):\n",
    "        return self.env.observe(self.currentAgent())[\"observation\"]\n",
    "\n",
    "    def currentAgent(self):\n",
    "        return self.env.agent_selection\n",
    "\n",
    "    def show(self, wait=False):\n",
    "        self.env.render()\n",
    "        if wait:\n",
    "            input(\"press any key to continue\")\n",
    "\n",
    "\n",
    "    def __eq__(self, x):\n",
    "        if not isinstance(x, State):\n",
    "            return False\n",
    "        # this should be enough\n",
    "        same_agent = self.env.agent_selection == x.env.agent_selection\n",
    "        observations_match = (self.getObservation() == x.getObservation()).all()\n",
    "        return same_agent and observations_match\n",
    "\n",
    "    def toStr(self):\n",
    "        o = self.getObservation()\n",
    "        # reduce dimensions from 3 to 2\n",
    "        o = np.sum(o, axis = 2) * (np.argmax(o, axis = 2) + 1)\n",
    "        return str(o)\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.toStr())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac4d7051",
   "metadata": {
    "id": "ac4d7051"
   },
   "outputs": [],
   "source": [
    "class TrainingExample:\n",
    "\n",
    "    def __init__(self, state : State, pi, reward):\n",
    "        self.state = state\n",
    "        self.pi = pi\n",
    "        self.reward = reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca0a680",
   "metadata": {
    "id": "4ca0a680"
   },
   "source": [
    "## The neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38bf156c",
   "metadata": {
    "id": "38bf156c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-14 18:47:17.802294: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-12-14 18:47:17.802314: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "import random\n",
    "from tensorflow.keras import layers, Model, Input, metrics, losses\n",
    "import tensorflow as tf\n",
    "\n",
    "def applyActionMaskToPolicy(p, action_mask):\n",
    "    p_masked = p * action_mask\n",
    "    # policy zeroed all possible actions\n",
    "    if np.sum(p_masked) == 0:\n",
    "        p_masked = m\n",
    "\n",
    "    return p_masked / np.sum(p_masked) # renormalize\n",
    "\n",
    "\n",
    "class NNet:\n",
    "\n",
    "    def __init__(self, action_size):\n",
    "        x = Input(shape=(8,8,4))\n",
    "        y = layers.Conv2D(32, 3, activation='relu')(x)\n",
    "        y = layers.Conv2D(32, 3, activation='relu')(y)\n",
    "        y = layers.Flatten()(y)\n",
    "        # y = layers.Dropout(0.5)(y)\n",
    "        p = layers.Dense(action_size, activation='softmax', name=\"p\")(y)\n",
    "        v = layers.Dense(1, name=\"v\")(y)\n",
    "        self.nnet = Model(x, [p,v])\n",
    "        print(self.nnet.summary())\n",
    "        \n",
    "        def entropyLoss(y_true, y_pred):\n",
    "            return -y_true * tf.math.log(y_pred + 1e-10)\n",
    "            \n",
    "        self.nnet.compile(\n",
    "                optimizer=\"adam\",\n",
    "                loss={\"p\": entropyLoss, \"v\":\"mse\"}\n",
    "        )\n",
    "\n",
    "    def predict(self, state : State):\n",
    "        x = state.getObservation()\n",
    "        x = np.expand_dims(x, 0)\n",
    "        p, v = self.nnet.predict(x, batch_size=1)\n",
    "\n",
    "        p = p[0]\n",
    "        p = applyActionMaskToPolicy(p, state.getActionMask())\n",
    "        return p, v[0][0]\n",
    "\n",
    "    @staticmethod\n",
    "    def _prepare_examples(examples: List[TrainingExample]):\n",
    "        X = []\n",
    "        pi = []\n",
    "        v = []\n",
    "        for e in examples:\n",
    "            X.append(e.state.getObservation())\n",
    "            pi.append(e.pi)\n",
    "            v.append(e.reward)\n",
    "        \n",
    "        return np.array(X), [np.array(pi), np.array(v)]\n",
    "     \n",
    "    def train(self, examples):\n",
    "        X, y = self._prepare_examples(examples)\n",
    "        self.nnet.fit(X, y, batch_size=32, shuffle=True, epochs=3)\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86286e92",
   "metadata": {
    "id": "86286e92"
   },
   "outputs": [],
   "source": [
    "class RandomPlayer:\n",
    "    \n",
    "    def predict(self, state : State):\n",
    "        p = np.random.uniform(256)\n",
    "        return applyActionMaskToPolicy(p, state.getActionMask()), 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae70eee",
   "metadata": {
    "id": "2ae70eee"
   },
   "source": [
    "Check if it works : create environment, wrap it in state, run nnet predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5bc3776",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d5bc3776",
    "outputId": "d389f35f-c4f6-404b-c6c6-68f188a3b505"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  M   M   M   M \n",
      "M   M   M   M   \n",
      "  M   M   M   M \n",
      "_   _   _   _   \n",
      "  _   _   _   _ \n",
      "m   m   m   m   \n",
      "  m   m   m   m \n",
      "m   m   m   m   \n"
     ]
    }
   ],
   "source": [
    "env = checkers_v3.env()\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f6c2c13",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4f6c2c13",
    "outputId": "87b45aa1-c5a4-4626-8f9c-fad76c767a9c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 8, 4)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = State(env)\n",
    "state.getObservation().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e33101b9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e33101b9",
    "outputId": "d1f7fb61-c994-4f65-8394-6db949a832d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 8, 8, 4)]    0           []                               \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 6, 6, 32)     1184        ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 4, 4, 32)     9248        ['conv2d[0][0]']                 \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 512)          0           ['conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " p (Dense)                      (None, 256)          131328      ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " v (Dense)                      (None, 1)            513         ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 142,273\n",
      "Trainable params: 142,273\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-14 18:47:19.661426: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-12-14 18:47:19.661466: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-12-14 18:47:19.661495: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (TARS): /proc/driver/nvidia/version does not exist\n",
      "2021-12-14 18:47:19.661796: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 444ms/step - loss: 0.9315 - p_loss: 0.0217 - v_loss: 0.9098\n",
      "Epoch 2/3\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7249 - p_loss: 0.0217 - v_loss: 0.7032\n",
      "Epoch 3/3\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5519 - p_loss: 0.0217 - v_loss: 0.5303\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.NNet at 0x7f43857c72b0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnet = NNet(256)\n",
    "examples = [TrainingExample(state, np.full(256, 1.0 / 256), 1) for _ in range(32)]\n",
    "nnet.train(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5391873",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a5391873",
    "outputId": "27d272bc-48fa-428c-a839-971b31ec3c0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256,)\n",
      "0.37934464\n"
     ]
    }
   ],
   "source": [
    "policy, value = nnet.predict(state)\n",
    "print(policy.shape)\n",
    "print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80ea4bdc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "80ea4bdc",
    "outputId": "6b0ea58b-2f6a-4596-c6bb-8d0aa555015a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256,)\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "policy, value = RandomPlayer().predict(state)\n",
    "print(policy.shape)\n",
    "print(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28eb3fe",
   "metadata": {
    "id": "d28eb3fe"
   },
   "source": [
    "## MCTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0bf98423",
   "metadata": {
    "id": "0bf98423"
   },
   "outputs": [],
   "source": [
    "class MCTSNode:\n",
    "\n",
    "    def __init__ (self, p, q):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        p : policy in this state\n",
    "        q : q value of this state\n",
    "        \"\"\"\n",
    "        self.p = p\n",
    "        self.q = q\n",
    "        # n[a] : number of times and action has been performed from this state\n",
    "        self.n = np.zeros(len(p))\n",
    "        # q_a : q values of states following performing an action a\n",
    "        self.q_a = np.zeros(len(p))\n",
    "\n",
    "\n",
    "class MCTS:\n",
    "\n",
    "    def __init__(self, nnet, num_mcts_sims, max_depth = 10):\n",
    "        self.nnet = nnet\n",
    "        self.nodes = {}\n",
    "        self.c_puct = 1.0\n",
    "        self.num_mcts_sims = num_mcts_sims\n",
    "        self.max_depth = max_depth\n",
    "        \n",
    "    def search(self, s : State):\n",
    "        for _ in range(self.num_mcts_sims):\n",
    "            self._search(s, self.max_depth)\n",
    "\n",
    "    def _search(self, s : State, max_depth):\n",
    "        if s.gameEnded(): return s.gameReward()\n",
    "\n",
    "        if s not in self.nodes:\n",
    "            p, v = self.nnet.predict(s)\n",
    "            self.nodes[s] = MCTSNode(p, v)\n",
    "            return v\n",
    "\n",
    "        node = self.nodes[s]\n",
    "\n",
    "        if max_depth == 0:\n",
    "            # max depth reached, returning a heuristic value of this state\n",
    "            return node.q\n",
    "      \n",
    "        # upper confidence bound\n",
    "        ucb = node.q_a + self.c_puct * node.p * np.sqrt(np.sum(node.n)) / (1 + node.n)\n",
    "        ucb[s.getActionMask() == 0] = -np.inf\n",
    "        # choose best action based on ucb\n",
    "        a = np.argmax(ucb)\n",
    "        \n",
    "        sp, player_changed = s.nextState(a)\n",
    "        v = self._search(sp, max_depth - 1)\n",
    "        if player_changed:\n",
    "            v = -v\n",
    "\n",
    "        node.q_a[a] = (node.n[a] * node.q_a[a] + v) / (node.n[a] + 1)\n",
    "        node.n[a] += 1\n",
    "        return v\n",
    "\n",
    "    # improved policy\n",
    "    def pi(self, s : State):\n",
    "        node = self.nodes[s]\n",
    "        n_sum = np.sum(node.n)\n",
    "        if n_sum == 0:\n",
    "            return node.p\n",
    "\n",
    "        return node.n / n_sum\n",
    "\n",
    "\n",
    "# mcts = MCTS(nnet, 30)\n",
    "# %timeit -r 2 -n 5 mcts.search(state)\n",
    "# old version time: 5 loops, best of 2: 4.6 s per loop\n",
    "# current time: 5 loops, best of 2: 1.54 s per loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe2f3915",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fe2f3915",
    "outputId": "69bd68fd-fa13-45af-ac74-f472edc10838"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mcts = MCTS(nnet, 2)\n",
    "mcts.search(state)\n",
    "mcts.pi(state).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4594b958",
   "metadata": {
    "id": "4594b958"
   },
   "outputs": [],
   "source": [
    "def pit(new_nnet : NNet, nnet : NNet, games_played = 40):\n",
    "    new_nnet_tag = \"player_0\"\n",
    "    nnet_tag = \"player_1\"\n",
    "    wins = 0\n",
    "    ties = 0\n",
    "\n",
    "    for g in range(games_played):\n",
    "        env = checkers_v3.env()\n",
    "        env.reset()\n",
    "        s = State(env)\n",
    "        # swap players before each round\n",
    "        new_nnet_tag, nnet_tag = nnet_tag, new_nnet_tag  \n",
    "        agents = {new_nnet_tag : new_nnet, nnet_tag : nnet}\n",
    "\n",
    "        while not s.gameEnded():\n",
    "            agent = agents[s.currentAgent()]\n",
    "            p, _ = agent.predict(s)\n",
    "            action = np.random.choice(len(p), p=p)\n",
    "            s.env.step(action)\n",
    "\n",
    "        if s.gameReward() == 0:\n",
    "            ties += 1\n",
    "       \n",
    "        if s.gameReward() == 1 and s.currentAgent() == new_nnet_tag:\n",
    "            wins += 1\n",
    "    \n",
    "        if s.gameReward() == -1 and s.currentAgent() != new_nnet_tag:\n",
    "            wins += 1\n",
    "        \n",
    "            \n",
    "    frac_win = wins / (games_played - ties)\n",
    "    return frac_win\n",
    "\n",
    "# training\n",
    "def policyIterSP(env : AECEnv, num_iters = 10, num_eps = 10,  num_mcts_sims=25, frac_win_thresh = 0.55):\n",
    "    # hard coded action space size\n",
    "    nnet = NNet(256)\n",
    "    frac_win = pit(nnet, RandomPlayer())                              # compare new net with a random player\n",
    "    print(\"frac_wins against a random player\", frac_win)\n",
    "    examples = []\n",
    "    for i in range(num_iters):\n",
    "        for e in range(num_eps):\n",
    "            examples += executeSelfPlayEpisode(env, nnet, num_mcts_sims)    # collect examples from this game\n",
    "            print(\"episode done\")\n",
    "        new_nnet = nnet.train(examples)\n",
    "        frac_win = pit(new_nnet, nnet)                                # compare new net with previous net\n",
    "        print(\"frac_win\", frac_win)\n",
    "        if frac_win > frac_win_thresh:\n",
    "            print(\"new net is better!\")\n",
    "            nnet = new_nnet                                           # replace with new net\n",
    "            frac_win = pit(nnet, RandomPlayer())                      # compare new net with a random player\n",
    "            print(\"frac_wins against a random player\", frac_win)\n",
    "        examples = random.sample(examples, len(examples) // 2)        # discard half of the examples\n",
    "    return nnet\n",
    "\n",
    "def executeSelfPlayEpisode(env : AECEnv, nnet, num_mcts_sims = 3):\n",
    "    examples = []\n",
    "    env.reset()\n",
    "    s = State(env)\n",
    "    # s.show(wait = False)\n",
    "    mcts = MCTS(nnet, num_mcts_sims)\n",
    "\n",
    "    while True:\n",
    "        mcts.search(s)\n",
    "        pi = mcts.pi(s)\n",
    "        examples.append(TrainingExample(deepcopy(s), pi, None))  # rewards can not be determined yet\n",
    "        a = np.random.choice(len(pi), p=pi)                      # sample action from improved policy\n",
    "        s, _ = s.nextState(a)\n",
    "        # s.show(wait = False)\n",
    "        if s.gameEnded():\n",
    "            examples = assignRewards(examples, s.gameReward(), s.currentAgent())\n",
    "            return examples\n",
    "\n",
    "def assignRewards(examples, reward, player_w_reward):\n",
    "    for e in examples:\n",
    "        e.reward = reward if e.state.currentAgent() == player_w_reward else -reward\n",
    "\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdaa9fa6",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2784af13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 8, 8, 4)]    0           []                               \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 6, 6, 32)     1184        ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 4, 4, 32)     9248        ['conv2d_2[0][0]']               \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 512)          0           ['conv2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " p (Dense)                      (None, 256)          131328      ['flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      " v (Dense)                      (None, 1)            513         ['flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 142,273\n",
      "Trainable params: 142,273\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "frac_wins against a random player 0.525\n",
      "episode done\n",
      "Epoch 1/3\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 1.0009 - p_loss: 0.0216 - v_loss: 0.9793\n",
      "Epoch 2/3\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.8792 - p_loss: 0.0215 - v_loss: 0.8577\n",
      "Epoch 3/3\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7808 - p_loss: 0.0213 - v_loss: 0.7595\n",
      "frac_win 0.625\n",
      "new net is better!\n",
      "frac_wins against a random player 0.6\n"
     ]
    }
   ],
   "source": [
    "env = checkers_v3.env()\n",
    "nnet = policyIterSP(env, num_iters=1, num_eps=1, num_mcts_sims=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b4518b",
   "metadata": {
    "id": "a7b4518b"
   },
   "source": [
    "## Run training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72c447cf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "72c447cf",
    "outputId": "5a1b54b5-4c73-4a72-d32d-34e394ee0d60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 8, 8, 4)]    0           []                               \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 6, 6, 32)     1184        ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 4, 4, 32)     9248        ['conv2d_4[0][0]']               \n",
      "                                                                                                  \n",
      " flatten_2 (Flatten)            (None, 512)          0           ['conv2d_5[0][0]']               \n",
      "                                                                                                  \n",
      " p (Dense)                      (None, 256)          131328      ['flatten_2[0][0]']              \n",
      "                                                                                                  \n",
      " v (Dense)                      (None, 1)            513         ['flatten_2[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 142,273\n",
      "Trainable params: 142,273\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "frac_wins against a random player 0.5\n",
      "episode done\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4182/140544722.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckers_v3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicyIterSP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_eps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_mcts_sims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_4182/2479556974.py\u001b[0m in \u001b[0;36mpolicyIterSP\u001b[0;34m(env, num_iters, num_eps, num_mcts_sims, frac_win_thresh)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_eps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mexamples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mexecuteSelfPlayEpisode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_mcts_sims\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# collect examples from this game\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"episode done\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mnew_nnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4182/2479556974.py\u001b[0m in \u001b[0;36mexecuteSelfPlayEpisode\u001b[0;34m(env, nnet, num_mcts_sims)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mmcts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0mpi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmcts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mexamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrainingExample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# rewards can not be determined yet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4182/4278009086.py\u001b[0m in \u001b[0;36msearch\u001b[0;34m(self, s)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mState\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_mcts_sims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mState\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4182/4278009086.py\u001b[0m in \u001b[0;36m_search\u001b[0;34m(self, s, max_depth)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0msp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplayer_changed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnextState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mplayer_changed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4182/4278009086.py\u001b[0m in \u001b[0;36m_search\u001b[0;34m(self, s, max_depth)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgameEnded\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgameReward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMCTSNode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4182/603673748.py\u001b[0m in \u001b[0;36m__eq__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;31m# this should be enough\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0msame_agent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent_selection\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent_selection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mobservations_match\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetObservation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetObservation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msame_agent\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mobservations_match\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4182/603673748.py\u001b[0m in \u001b[0;36mgetObservation\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgetObservation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobserve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrentAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"observation\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcurrentAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/checkers-ai/.venv/lib/python3.8/site-packages/pettingzoo/utils/wrappers/order_enforcing.py\u001b[0m in \u001b[0;36mobserve\u001b[0;34m(self, agent)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_reset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mEnvLogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_observe_before_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobserve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/checkers-ai/.venv/lib/python3.8/site-packages/pettingzoo/utils/wrappers/base.py\u001b[0m in \u001b[0;36mobserve\u001b[0;34m(self, agent)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mobserve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobserve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/checkers-ai/.venv/lib/python3.8/site-packages/pettingzoo/utils/wrappers/base.py\u001b[0m in \u001b[0;36mobserve\u001b[0;34m(self, agent)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mobserve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobserve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/checkers-ai/.venv/lib/python3.8/site-packages/pettingzoo/utils/wrappers/terminate_illegal.py\u001b[0m in \u001b[0;36mobserve\u001b[0;34m(self, agent)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mobserve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobserve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0magent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent_selection\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prev_obs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/checkers-ai/.venv/lib/python3.8/site-packages/pettingzoo/utils/wrappers/base.py\u001b[0m in \u001b[0;36mobserve\u001b[0;34m(self, agent)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mobserve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobserve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/checkers-ai/.venv/lib/python3.8/site-packages/pettingzoo/utils/wrappers/base.py\u001b[0m in \u001b[0;36mobserve\u001b[0;34m(self, agent)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mobserve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobserve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/checkers-ai/.venv/lib/python3.8/site-packages/pettingzoo/classic/checkers/checkers.py\u001b[0m in \u001b[0;36mobserve\u001b[0;34m(self, agent)\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msq\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m                     \u001b[0mobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msq\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = checkers_v3.env()\n",
    "nnet = policyIterSP(env, num_iters=8, num_eps=50, num_mcts_sims=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dba3eb",
   "metadata": {
    "id": "31dba3eb"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "self_play.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "checkers",
   "language": "python",
   "name": "checkers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
